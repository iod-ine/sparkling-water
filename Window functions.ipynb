{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59112184",
   "metadata": {},
   "source": [
    "## Window functions\n",
    "\n",
    "- Windowed functions fill the niche between `.groupBy().agg()` and group map UDFs.\n",
    "- Windows determine which records are used for the application of a function.\n",
    "- Windowed functions preserve the number of records in the data frame: the results of aggregations are broadcasted to all the rows of the group. \n",
    "- `pyspark.sql.window.Window` is the builder class for windows, which are represented by the `WindowSpec` objects. \n",
    "- `Window.partitionBy(*columns)` will create partitions for values of the columns. \n",
    "- To apply a function in the defined window, the `.over(window)` method of a `Column` is used.\n",
    "- Window functions are an elegant way to avoid self-joins. \n",
    "- Ranking functions rank records based on the value of a field. \n",
    "- Windows have an `.orderBy()` method that will sort the records within each window partition.\n",
    "- `F.rank().over(ordered_window)` will rank the values in the window partition according to the column used in `.orderBy()\n",
    "- Some ranking functions:\n",
    "  - `F.rank()` is a nonconsecutive rank: same values result in the same rank, the next value after duplicates is offset by the number of duplicates. \n",
    "  - `F.dense_rank()` is a dense rank: ties will still have the same rank, but there will be no skips. \n",
    "  - `F.percent_rank()` computes `number_of_records_smaller_than_current / (number_of_records_in_window - 1)`.\n",
    "  - `F.ntile()` creates an arbitrary number of tiles based on the rank of the data (i.e., quartiles, percentiles). \n",
    "  - `F.row_number()` will generate a row number regardless of ties.\n",
    "- The `.orderBy()` of the window does not have the `ascending` parameter. `Column.desc()` should be used instead. \n",
    "- `F.lag()` and `F.lead()` will get shifted values of the column.\n",
    "- `F.cume_dist()` computes the cumulative density function: `number_of_records_le_current / number_of_records_in_window`. \n",
    "- Boundaries of a window are called a *window frame*.\n",
    "- The boundaries are added to a window definition using `.rowsBetween()` and `.rangeBetween()` methods.\n",
    "- A *growing window* is a window unbounded on one of the sides: the computation uses a different number of records for each row. A *static window* is a window where both ends are bounded relatively to the current row, i.e. `.rowsBetween(-1, 1)`.\n",
    "- Spark refers to a record being processed within a window as `Window.currentRow`. The rows before take the values from `-1` to `Window.unboundedPreceeding` at the beginning of the window. Similarly, the rows after take values from `1` to `Window.unboundedFollowing`.\n",
    "- This can be used to create features that only look at the past and avoid leakage. \n",
    "- When the ordering is not defined for a window, an unbounded frame is used by default: all values in the window are included for each row. When ordering is defined, a growing window is used by default: for each row, only the rows up to it are used in the computation. **If partial aggregation is not required, the window must be unordered!**\n",
    "- *Row windows* look at the position of the rows while *range windows* look at the value of the column used to order. Ranges are useful for working with dates and times. Multiple observations of the given time will break the row window.\n",
    "- `F.unix_timestamp()` converts a date into integer number of seconds since the Unix epoch. Might be useful in range windows because they only work on numeric values.\n",
    "- `F.to_date()` creates a date. \n",
    "- `.rangeBetween()` is measured from the current row value.\n",
    "- Series-to-scalar and group-aggregate Pandas UDFs can be used over windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5f3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a541607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/29 15:28:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Window functions\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116fc5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://loki:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Window functions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3d9716fcd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf473c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5ad156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stn: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- mo: string (nullable = true)\n",
      " |-- da: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data/gsod_noaa/\")\n",
    "df = df.filter(F.col(\"stn\").isin([998252, 949110]))  # N = 718\n",
    "df = df.filter(F.col(\"year\") == 2019)\n",
    "df = df.select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbf557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+\n",
      "|stn   |year|mo |da |temp|\n",
      "+------+----+---+---+----+\n",
      "|949110|2019|11 |23 |54.9|\n",
      "|998252|2019|04 |18 |44.7|\n",
      "|998252|2019|06 |15 |53.3|\n",
      "|949110|2019|11 |26 |54.7|\n",
      "|949110|2019|11 |01 |69.9|\n",
      "+------+----+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef5b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:========>                                                (2 + 11) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----+----+----+----+----+----------+----------+----+\n",
      "|   stn|year| mo| da|temp|   a|   b|   c|   d|   e|        dt|     epoch|   f|\n",
      "+------+----+---+---+----+----+----+----+----+----+----------+----------+----+\n",
      "|949110|2019| 01| 01|65.8|75.9|65.8|65.8|75.9|65.8|2019-01-01|1546290000|65.8|\n",
      "|998252|2019| 01| 01|38.8|75.9|65.8|65.8|75.9|65.8|2019-01-01|1546290000|65.8|\n",
      "|949110|2019| 01| 02|61.6|75.9|65.8|65.8|75.9|71.3|2019-01-02|1546376400|65.8|\n",
      "|998252|2019| 01| 02|21.3|75.9|65.8|65.8|75.9|71.3|2019-01-02|1546376400|65.8|\n",
      "|949110|2019| 01| 03|71.3|75.9|71.3|71.3|75.9|74.1|2019-01-03|1546462800|71.3|\n",
      "|998252|2019| 01| 03|27.4|75.9|71.3|71.3|75.9|74.1|2019-01-03|1546462800|71.3|\n",
      "|949110|2019| 01| 04|74.1|75.9|74.1|74.1|75.9|74.1|2019-01-04|1546549200|74.1|\n",
      "|998252|2019| 01| 04|32.0|75.9|74.1|74.1|75.9|74.1|2019-01-04|1546549200|74.1|\n",
      "|998252|2019| 01| 05|36.3|75.9|74.1|74.1|75.9|63.9|2019-01-05|1546635600|74.1|\n",
      "|949110|2019| 01| 05|59.8|75.9|74.1|74.1|75.9|74.1|2019-01-05|1546635600|74.1|\n",
      "|998252|2019| 01| 06|34.3|75.9|74.1|74.1|75.9|65.5|2019-01-06|1546722000|74.1|\n",
      "|949110|2019| 01| 06|63.9|75.9|74.1|74.1|75.9|65.5|2019-01-06|1546722000|74.1|\n",
      "|949110|2019| 01| 07|65.5|75.9|74.1|74.1|75.9|65.5|2019-01-07|1546808400|74.1|\n",
      "|998252|2019| 01| 07|19.3|75.9|74.1|74.1|75.9|65.5|2019-01-07|1546808400|74.1|\n",
      "|949110|2019| 01| 08|64.1|75.9|74.1|74.1|75.9|65.5|2019-01-08|1546894800|65.5|\n",
      "|998252|2019| 01| 08|25.0|75.9|74.1|74.1|75.9|64.1|2019-01-08|1546894800|65.5|\n",
      "|949110|2019| 01| 09|59.7|75.9|74.1|74.1|75.9|64.3|2019-01-09|1546981200|65.5|\n",
      "|998252|2019| 01| 09|39.1|75.9|74.1|74.1|75.9|64.3|2019-01-09|1546981200|65.5|\n",
      "|949110|2019| 01| 10|64.3|75.9|74.1|74.1|75.9|72.3|2019-01-10|1547067600|65.5|\n",
      "|998252|2019| 01| 10|30.5|75.9|74.1|74.1|75.9|72.3|2019-01-10|1547067600|65.5|\n",
      "|998252|2019| 01| 11|18.8|75.9|74.1|74.1|75.9|72.3|2019-01-11|1547154000|72.3|\n",
      "|949110|2019| 01| 11|72.3|75.9|74.1|74.1|75.9|72.3|2019-01-11|1547154000|72.3|\n",
      "|949110|2019| 01| 12|61.2|75.9|74.1|74.1|75.9|72.3|2019-01-12|1547240400|72.3|\n",
      "|998252|2019| 01| 12|10.8|75.9|74.1|74.1|75.9|70.0|2019-01-12|1547240400|72.3|\n",
      "|998252|2019| 01| 13|13.2|75.9|74.1|74.1|75.9|71.8|2019-01-13|1547326800|72.3|\n",
      "|949110|2019| 01| 13|70.0|75.9|74.1|74.1|75.9|71.8|2019-01-13|1547326800|72.3|\n",
      "|949110|2019| 01| 14|71.8|75.9|74.1|74.1|75.9|71.8|2019-01-14|1547413200|72.3|\n",
      "|998252|2019| 01| 14|16.5|75.9|74.1|74.1|75.9|71.8|2019-01-14|1547413200|72.3|\n",
      "|949110|2019| 01| 15|70.6|75.9|74.1|74.1|75.9|71.8|2019-01-15|1547499600|71.8|\n",
      "|998252|2019| 01| 15|20.4|75.9|74.1|74.1|75.9|70.6|2019-01-15|1547499600|71.8|\n",
      "|949110|2019| 01| 16|70.3|75.9|74.1|74.1|75.9|75.6|2019-01-16|1547586000|71.8|\n",
      "|998252|2019| 01| 16|30.7|75.9|74.1|74.1|75.9|75.6|2019-01-16|1547586000|71.8|\n",
      "|949110|2019| 01| 17|75.6|75.9|75.6|75.6|75.9|75.6|2019-01-17|1547672400|75.6|\n",
      "+------+----+---+---+----+----+----+----+----+----+----------+----------+----+\n",
      "only showing top 33 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "a = Window.partitionBy(\"mo\")\n",
    "b = Window.partitionBy(\"mo\").orderBy(\"da\")\n",
    "c = Window.partitionBy(\"mo\").orderBy(\"da\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "d = Window.partitionBy(\"mo\").orderBy(\"da\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "e = Window.partitionBy(\"mo\").orderBy(\"da\").rowsBetween(-2, 2)\n",
    "\n",
    "three_days_sec = 3600 * 24 * 3\n",
    "f = Window.partitionBy(\"mo\").orderBy(\"epoch\").rangeBetween(-three_days_sec, Window.currentRow)\n",
    "\n",
    "\n",
    "res = (\n",
    "    df\n",
    "    .withColumn(\"a\", F.max(\"temp\").over(a))\n",
    "    .withColumn(\"b\", F.max(\"temp\").over(b))\n",
    "    .withColumn(\"c\", F.max(\"temp\").over(c))\n",
    "    .withColumn(\"d\", F.max(\"temp\").over(d))\n",
    "    .withColumn(\"e\", F.max(\"temp\").over(e))\n",
    "    .withColumn(\"dt\", F.to_date(F.concat_ws(\"-\", \"year\", \"mo\", \"da\")))\n",
    "    .withColumn(\"epoch\", F.unix_timestamp(\"dt\"))\n",
    "    .withColumn(\"f\", F.max(\"temp\").over(f))\n",
    ")\n",
    "\n",
    "res.orderBy(\"mo\", \"da\").show(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f09701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "43b2ac67b9055ea5f1b71b65016d46adeb9977e66ee43e4dcef19e1e023b065f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
