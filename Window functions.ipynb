{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59112184",
   "metadata": {},
   "source": [
    "## Window functions\n",
    "\n",
    "- Windowed functions fill the niche between `.groupBy().agg()` and group map UDFs.\n",
    "- Windows determine which records are used for the application of a function.\n",
    "- Windowed functions preserve the number of records in the data frame: the results of aggregations are broadcasted to all the rows of the group. \n",
    "- `pyspark.sql.window.Window` is the builder class for windows, which are represented by the `WindowSpec` objects. \n",
    "- `Window.partitionBy(*columns)` will create partitions for values of the columns. \n",
    "- To apply a function in the defined window, the `.over(window)` method of a `Column` is used.\n",
    "- Window functions are an elegant way to avoid self-joins. \n",
    "- Ranking functions rank records based on the value of a field. \n",
    "- Windows have an `.orderBy()` method that will sort the records within each window partition.\n",
    "- `F.rank().over(ordered_window)` will rank the values in the window partition according to the column used in `.orderBy()\n",
    "- Some ranking functions:\n",
    "  - `F.rank()` is a nonconsecutive rank: same values result in the same rank, the next value after duplicates is offset by the number of duplicates. \n",
    "  - `F.dense_rank()` is a dense rank: ties will still have the same rank, but there will be no skips. \n",
    "  - `F.percent_rank()` computes `number_of_records_smaller_than_current / (number_of_records_in_window - 1)`.\n",
    "  - `F.ntile()` creates an arbitrary number of tiles based on the rank of the data (i.e., quartiles, percentiles). \n",
    "  - `F.row_number()` will generate a row number regardless of ties.\n",
    "- The `.orderBy()` of the window does not have the `ascending` parameter. `Column.desc()` should be used instead. \n",
    "- `F.lag()` and `F.lead()` will get shifted values of the column.\n",
    "- `F.cume_dist()` computes the cumulative density function: `number_of_records_le_current / number_of_records_in_window`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5f3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a541607",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Window functions\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116fc5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://loki:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Window functions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f825d3cb520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf473c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be5ad156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stn: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- mo: string (nullable = true)\n",
      " |-- da: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/user/ivan.dubrovin/gsod_noaa_2019_\")\n",
    "df = df.filter(F.col(\"stn\").isin([998252, 949110]))  # N = 365\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecbf557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+\n",
      "|stn   |year|mo |da |temp|\n",
      "+------+----+---+---+----+\n",
      "|949110|2019|01 |13 |70.0|\n",
      "|949110|2019|01 |26 |66.8|\n",
      "|949110|2019|11 |05 |54.3|\n",
      "|949110|2019|12 |13 |57.3|\n",
      "|949110|2019|01 |10 |64.3|\n",
      "+------+----+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5ef5b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----+----+----+----+----+----------+----------+----+\n",
      "|   stn|year| mo| da|temp|   a|   b|   c|   d|   e|        dt|     epoch|   f|\n",
      "+------+----+---+---+----+----+----+----+----+----+----------+----------+----+\n",
      "|949110|2019| 01| 01|65.8|75.9|65.8|65.8|75.9|71.3|2019-01-01|1546290000|65.8|\n",
      "|949110|2019| 01| 02|61.6|75.9|65.8|65.8|75.9|74.1|2019-01-02|1546376400|65.8|\n",
      "|949110|2019| 01| 03|71.3|75.9|71.3|71.3|75.9|74.1|2019-01-03|1546462800|71.3|\n",
      "|949110|2019| 01| 04|74.1|75.9|74.1|74.1|75.9|74.1|2019-01-04|1546549200|74.1|\n",
      "|949110|2019| 01| 05|59.8|75.9|74.1|74.1|75.9|74.1|2019-01-05|1546635600|74.1|\n",
      "|949110|2019| 01| 06|63.9|75.9|74.1|74.1|75.9|74.1|2019-01-06|1546722000|74.1|\n",
      "|949110|2019| 01| 07|65.5|75.9|74.1|74.1|75.9|65.5|2019-01-07|1546808400|74.1|\n",
      "|949110|2019| 01| 08|64.1|75.9|74.1|74.1|75.9|65.5|2019-01-08|1546894800|65.5|\n",
      "|949110|2019| 01| 09|59.7|75.9|74.1|74.1|75.9|72.3|2019-01-09|1546981200|65.5|\n",
      "|949110|2019| 01| 10|64.3|75.9|74.1|74.1|75.9|72.3|2019-01-10|1547067600|65.5|\n",
      "|949110|2019| 01| 11|72.3|75.9|74.1|74.1|75.9|72.3|2019-01-11|1547154000|72.3|\n",
      "|949110|2019| 01| 12|61.2|75.9|74.1|74.1|75.9|72.3|2019-01-12|1547240400|72.3|\n",
      "|949110|2019| 01| 13|70.0|75.9|74.1|74.1|75.9|72.3|2019-01-13|1547326800|72.3|\n",
      "|949110|2019| 01| 14|71.8|75.9|74.1|74.1|75.9|71.8|2019-01-14|1547413200|72.3|\n",
      "|949110|2019| 01| 15|70.6|75.9|74.1|74.1|75.9|75.6|2019-01-15|1547499600|71.8|\n",
      "|949110|2019| 01| 16|70.3|75.9|74.1|74.1|75.9|75.6|2019-01-16|1547586000|71.8|\n",
      "|949110|2019| 01| 17|75.6|75.9|75.6|75.6|75.9|75.6|2019-01-17|1547672400|75.6|\n",
      "|949110|2019| 01| 18|67.2|75.9|75.6|75.6|75.9|75.6|2019-01-18|1547758800|75.6|\n",
      "|949110|2019| 01| 19|63.8|75.9|75.6|75.6|75.9|75.6|2019-01-19|1547845200|75.6|\n",
      "|949110|2019| 01| 20|65.9|75.9|75.6|75.6|75.9|67.7|2019-01-20|1547931600|75.6|\n",
      "|949110|2019| 01| 21|67.0|75.9|75.6|75.6|75.9|69.5|2019-01-21|1548018000|67.2|\n",
      "|949110|2019| 01| 22|67.7|75.9|75.6|75.6|75.9|75.9|2019-01-22|1548104400|67.7|\n",
      "|949110|2019| 01| 23|69.5|75.9|75.6|75.6|75.9|75.9|2019-01-23|1548190800|69.5|\n",
      "|949110|2019| 01| 24|75.9|75.9|75.9|75.9|75.9|75.9|2019-01-24|1548277200|75.9|\n",
      "|949110|2019| 01| 25|71.5|75.9|75.9|75.9|71.5|75.9|2019-01-25|1548363600|75.9|\n",
      "|949110|2019| 01| 26|66.8|75.9|75.9|75.9|71.0|75.9|2019-01-26|1548450000|75.9|\n",
      "|949110|2019| 01| 27|65.4|75.9|75.9|75.9|71.0|71.5|2019-01-27|1548536400|75.9|\n",
      "|949110|2019| 01| 28|68.5|75.9|75.9|75.9|71.0|71.0|2019-01-28|1548622800|71.5|\n",
      "|949110|2019| 01| 29|69.8|75.9|75.9|75.9|71.0|71.0|2019-01-29|1548709200|69.8|\n",
      "|949110|2019| 01| 30|71.0|75.9|75.9|75.9|71.0|71.0|2019-01-30|1548795600|71.0|\n",
      "|949110|2019| 01| 31|60.0|75.9|75.9|75.9|60.0|71.0|2019-01-31|1548882000|71.0|\n",
      "|949110|2019| 02| 01|67.9|75.7|67.9|67.9|75.7|73.6|2019-02-01|1548968400|67.9|\n",
      "|949110|2019| 02| 02|73.6|75.7|73.6|73.6|75.7|73.6|2019-02-02|1549054800|73.6|\n",
      "+------+----+---+---+----+----+----+----+----+----+----------+----------+----+\n",
      "only showing top 33 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = Window.partitionBy(\"mo\")\n",
    "b = Window.partitionBy(\"mo\").orderBy(\"da\")\n",
    "c = Window.partitionBy(\"mo\").orderBy(\"da\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "d = Window.partitionBy(\"mo\").orderBy(\"da\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "e = Window.partitionBy(\"mo\").orderBy(\"da\").rowsBetween(-2, 2)\n",
    "\n",
    "three_days_sec = 3600 * 24 * 3\n",
    "f = Window.partitionBy(\"mo\").orderBy(\"epoch\").rangeBetween(-three_days_sec, Window.currentRow)\n",
    "\n",
    "\n",
    "res = (\n",
    "    df\n",
    "    .withColumn(\"a\", F.max(\"temp\").over(a))\n",
    "    .withColumn(\"b\", F.max(\"temp\").over(b))\n",
    "    .withColumn(\"c\", F.max(\"temp\").over(c))\n",
    "    .withColumn(\"d\", F.max(\"temp\").over(d))\n",
    "    .withColumn(\"e\", F.max(\"temp\").over(e))\n",
    "    .withColumn(\"dt\", F.to_date(F.concat_ws(\"-\", \"year\", \"mo\", \"da\")))\n",
    "    .withColumn(\"epoch\", F.unix_timestamp(\"dt\"))\n",
    "    .withColumn(\"f\", F.max(\"temp\").over(f))\n",
    ")\n",
    "\n",
    "res.orderBy(\"mo\", \"da\").show(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f09701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
